# -*- coding: utf-8 -*-
"""NLI with XLMRobertaModel

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nli-with-xlmrobertamodel-0a1532da-cb8b-4d2a-b379-489544ee7cfa.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240911/auto/storage/goog4_request%26X-Goog-Date%3D20240911T061812Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3e25a849162eb5ab3fef38a3841450d9af0074e0b4299c6da8e89fd70a1bfe5b4ec625a8af7d154f45889d121c551407bf543a28a6c6bb7c99b61057b0ed661c9a349d0cc8664ca7346a2d0f483edbeda0639af9cebdf6588c4fc159e486fc300dc194889df76ad9d29c0822d940878be2528d6df3c8b197ba84784797043e2d8bbeca461b3e9d2fb0d7ccab00f7ff14811ea00dce5fa8b2f4c47ef33f6c517fc3ad44293fbdd408b1d677a2b3352bae3831add6a6d553b548423f29d4744066c7b8eaec334d83453523e541214b143d99d5603f3c2faf6df01bf5b5d9971622a22dfd7fc7bb4e3f4980ceb3166ebf2bbbaae9f93c6f25602365cef7efc4c6ad
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'contradictory-my-dear-watson:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F21733%2F1408234%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240911%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240911T061812Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc1feea250d33136185e910776da86f8d5caba5cd24fab090df10195be186d88958c70fa693ca03bcaf3456ea6be314bf2284249e795625241823bf9305f72596653865ab35bae7dbc5cd0f87c293e706f439a41cc44169ae19480477038523c5a94affd91c68987a091c35f56e8a94e61584053d925b76bdd8147122213ccc9e1f7a54c8bd2dc80341520ae0058488092fcec3d79d344d4d1f58650595c16fbffa513e556b719be5d14550afda04f6a58e86c4760373f510ade64a7c07b57d04231893cde1c4cd83c2204994f7cc4dc0e1887720b89033b4514d85a6f40daabb4aa5e4868cfbc41905f16a61ead77cd758ee328903cd2e3f52e21f4697854ddf'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

!pip install datasets
!pip install sentencepiece
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
import matplotlib.pyplot as plt
import torch
from datasets import load_dataset
import gc
import os
import pandas as pd
import numpy as np
import sentencepiece as spm

class CFG:
    val_split_ratio = 0.5
    batch_size = 32
    lr = 1e-5
    num_epochs = 3
    num_labels = 3

train = pd.read_csv("../input/contradictory-my-dear-watson/train.csv")
test = pd.read_csv("/kaggle/input/contradictory-my-dear-watson/test.csv")

train.head()

train.shape

train.info()

train.premise.values[1]

train.hypothesis.values[1]

train.label.values[1]

labels, frequencies = np.unique(train.language.values, return_counts = True)

plt.figure(figsize = (10,10))
plt.pie(frequencies,labels = labels, autopct = '%1.1f%%')
plt.show()

labels, frequencies = np.unique(test.language.values, return_counts = True)

plt.figure(figsize = (10,10))
plt.pie(frequencies,labels = labels, autopct = '%1.1f%%')
plt.show()

train['lang_abv'].nunique(), test['lang_abv'].nunique()

#same languages in test and train
len(set(train['lang_abv']).intersection(set(test['lang_abv'])))

train['label'].value_counts()

def load_mnli(use_validation=True):
    result = []
    dataset = load_dataset('multi_nli')
    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']
    for k in keys:
        for record in dataset[k]:
            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']
            if c1 and c2 and c3 in {0,1,2}:
                result.append((c1,c2,c3,'en'))
    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])
    return result.iloc[:30000, :]

def load_xnli():
    result = []
    dataset = load_dataset('xnli', 'all_languages')
    for k in dataset.keys():
        for record in dataset[k]:
            hp, pr, lb = record['hypothesis'], record['premise'], record['label']
            if hp and pr and lb in {0,1,2}:
                for lang, translation in zip(hp['language'], hp['translation']):
                    pr_lang = pr.get(lang, None)
                    if pr_lang is None:
                        continue
                    result.append((pr_lang, translation, lb,lang))
    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])
    return result

#load additional data(30k batch)
mnli = load_mnli()
# xnli = load_xnli()

mnli.head()

# xnli.head()

from sklearn.model_selection import train_test_split

# Split the original train data
train_premises, val_premises, train_hypotheses, val_hypotheses, train_labels, val_labels = train_test_split(
    train.premise.tolist(), train.hypothesis.tolist(), train.label.tolist(), test_size=CFG.val_split_ratio, random_state=42
)

# Add MNLI data to training set
train_premises += mnli.premise.tolist()
train_hypotheses += mnli.hypothesis.tolist()
train_labels += mnli.label.tolist()

# # Add XNLI data to training set
# train_premises += xnli.premise.tolist()
# train_hypotheses += xnli.hypothesis.tolist()
# train_labels += xnli.label.tolist()

# Clean up to free memory
del mnli
# del xnli
gc.collect()

print(f"Training set size: {len(train_labels)}")
print(f"Validation set size: {len(val_labels)}")

model_name = 'xlm-roberta-base'

tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Tokenize the training data
# 
# train_inputs = tokenizer(train_premises, train_hypotheses, padding=True, truncation=True, return_tensors='pt')
# train_labels = torch.tensor(train_labels)
# 
# # Tokenize the validation data
# val_inputs = tokenizer(val_premises, val_hypotheses, padding=True, truncation=True, return_tensors='pt')
# val_labels = torch.tensor(val_labels)

from torch.utils.data import DataLoader, TensorDataset

# Create TensorDatasets
train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, num_workers=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, num_workers=2)

from torch.optim import AdamW
from transformers import get_scheduler

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=CFG.num_labels)
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs for training.")
    model = torch.nn.DataParallel(model, device_ids=[0, 1])

model.to(device)

optimizer = AdamW(model.parameters(), lr=CFG.lr)

# Set up the learning rate scheduler
num_epochs = CFG.num_epochs
num_training_steps = num_epochs * len(train_loader)
criterion = torch.nn.CrossEntropyLoss()
lr_scheduler = get_scheduler(
    "linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

model.train()

for epoch in range(num_epochs):
    # Training
    model.train()
    total_train_loss = 0
    i = 1
    for batch in train_loader:
        input_ids, attention_mask, labels = batch

        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        labels = labels.to(device)

        # Clear previous gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = criterion(outputs.logits, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        total_train_loss += loss.item()
        if i % 200 == 0:
            print(f"Epoch number: {epoch}, Steps per epoch: {i}, avg train loss: {round(total_train_loss / i, 2)}")
        i += 1

    avg_train_loss = total_train_loss / len(train_loader)

    # Validation
    model.eval()
    total_val_loss = 0
    correct_predictions = 0

    with torch.no_grad():
        for batch in val_loader:
            input_ids, attention_mask, labels = batch

            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = criterion(outputs.logits, labels)
            total_val_loss += loss.item()

            # Get predictions
            predictions = torch.argmax(outputs.logits, dim=-1)
            correct_predictions += (predictions == labels).sum().item()

    avg_val_loss = total_val_loss / len(val_loader)
    val_accuracy = correct_predictions / len(val_dataset)

    print(f"Epoch {epoch + 1}/{num_epochs}")
    print(f"Training Loss: {avg_train_loss:.4f}")
    print(f"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\n")

test_inputs = tokenizer(test.premise.tolist(), test.hypothesis.tolist(), padding=True, truncation=True, return_tensors='pt')

test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])
test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, num_workers=2)
predictions = []
# Inference
model.eval()
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask = batch
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=-1).detach().cpu().tolist()
        predictions.append(preds)

import itertools

# Flattening the 2D list
predictions = list(itertools.chain(*predictions))

submission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')
submission['prediction'] = predictions

submission.head()

submission.to_csv("submission.csv", index = False)